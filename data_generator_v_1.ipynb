{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8283a05b-1311-4bd5-a386-fe3d51dcf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITERATION-D Synthetic Dataset Generator (Notebook-Friendly)\n",
    "# Project: ITERATION-D – CAPES-STIC-AmSud\n",
    "\n",
    "# Importações essenciais para geração de números aleatórios\n",
    "# e manipulação de tabelas (DataFrames).\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb4afd-f7d9-419b-9bd1-86ec6cd36ca4",
   "metadata": {},
   "source": [
    "## Configuração dos cenários ##\n",
    "A função abaixo, \"get_scenario_config\", é responsável por:\n",
    "- Criar um dicionário de parâmetros que representa o cenário escolhido.\n",
    "- Indicar se há desastre, flapping, saturação, recuperação adaptativa etc.\n",
    "- Definir os tempos de início e fim do desastre com base na duração total.\n",
    "- Ser consultada pelas demais partes do código para ajustar a geração das métricas.\n",
    "É aqui que as configurações de cada cenário são definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99e7284e-9b07-414b-bd19-6a8d49fcd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_config(scenario, duration):\n",
    "    cfg = {\n",
    "        \"name\": scenario,\n",
    "        \"disaster\": True,\n",
    "        \"disaster_start\": int(duration * 0.33),\n",
    "        \"disaster_duration\": int(duration * 0.33),\n",
    "        \"flapping\": False,\n",
    "        \"high_traffic\": False,\n",
    "        \"adaptive_recovery\": False,\n",
    "        \"stable\": False,\n",
    "    }\n",
    "\n",
    "    if scenario == \"C1\":\n",
    "        cfg[\"disaster\"] = False\n",
    "        cfg[\"stable\"] = True\n",
    "\n",
    "    elif scenario == \"C3\":\n",
    "        cfg[\"adaptive_recovery\"] = True\n",
    "\n",
    "    elif scenario == \"C4\":\n",
    "        cfg[\"high_traffic\"] = True\n",
    "\n",
    "    elif scenario == \"C5\":\n",
    "        cfg[\"flapping\"] = True\n",
    "\n",
    "    if cfg[\"disaster\"]:\n",
    "        cfg[\"disaster_end\"] = cfg[\"disaster_start\"] + cfg[\"disaster_duration\"]\n",
    "    else:\n",
    "        cfg[\"disaster_start\"] = None\n",
    "        cfg[\"disaster_end\"] = None\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9ffc6-f781-42df-80c9-6a865526c3ec",
   "metadata": {},
   "source": [
    "## Visão geral dos cenários ##\n",
    "\n",
    "1) No cenário C1 (Estável):\n",
    "não há desastre,\n",
    "não há flapping,\n",
    "não há congestionamento,\n",
    "não há recuperação adaptativa,\n",
    "a rede opera continuamente em \"normal\".\n",
    "\n",
    "2) No cenário C2 (Desastre Natural):\n",
    "herda o comportamento padrão (“disaster=True”),\n",
    "tem uma fase pre → during → post,\n",
    "durante o desastre veremos as maiores degradações.\n",
    "\n",
    "\n",
    "3) No cenário C3 (Recuperação Adaptativa):\n",
    "igual ao C2, mas com melhoria progressiva no post-disaster.\n",
    "\n",
    "\n",
    "4) No cenário C4 (Saturação Urbana):\n",
    "aumenta significativamente a ocupação de fila,\n",
    "aumenta perda\n",
    "aumenta jitter e atraso.\n",
    "\n",
    "\n",
    "5) No cenário C5 (Flapping):\n",
    "links alternam up/down,\n",
    "perda atinge 100% quando o link está down,\n",
    "latência dobra ao voltar (simulando backlog)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74d209-0032-4365-b20b-06e885193354",
   "metadata": {},
   "source": [
    "## Introdução à Parte 1 dos Geradores ##\n",
    "Nesta seção, vamos analisar como a ferramenta constrói a topologia estática da rede.\n",
    "- Nós: edge, fog, cloud\n",
    "- Enlaces entre nós: latência base, banda e perda base\n",
    "- A lógica temporal do cenário\n",
    "- Como identificar se um instante pertence à fase pre/during/post-disaster\n",
    "- Como essa função é utilizada depois nas séries temporais\n",
    "Esses elementos são fundamentais, pois toda a simulação é construída sobre:\n",
    "- onde estão os nós,\n",
    "- como eles se conectam,\n",
    "- quais são os parâmetros físicos dessas conexões,\n",
    "- em que fase da simulação nos encontramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a278035e-88d2-40c4-857f-f2d3fafa79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nodes(n_edge, n_fog, n_cloud):\n",
    "    device_types = [\"sensor\", \"UAV\", \"wearable\"]\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n_edge):\n",
    "        rows.append([\n",
    "            f\"E{i+1}\",      # id\n",
    "            \"edge\",         # layer\n",
    "            random.choice(device_types),  # device_type (NÃO É role!)\n",
    "            random.choice([\"urbano\",\"rural\"]),\n",
    "            \"low\",\n",
    "            1\n",
    "        ])\n",
    "\n",
    "    for i in range(n_fog):\n",
    "        rows.append([\n",
    "            f\"F{i+1}\",\n",
    "            \"fog\",\n",
    "            \"gateway\",\n",
    "            random.choice([\"urbano\",\"rural\"]),\n",
    "            \"medium\",\n",
    "            1\n",
    "        ])\n",
    "\n",
    "    for i in range(n_cloud):\n",
    "        rows.append([\n",
    "            f\"C{i+1}\",\n",
    "            \"cloud\",\n",
    "            \"datacenter\",\n",
    "            \"-\",\n",
    "            \"high\",\n",
    "            1\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"node_id\",\"layer\",\"device_type\",\"region\",\n",
    "        \"compute_capacity\",\"is_critical\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365db73e-8164-47f6-997d-379d995be30e",
   "metadata": {},
   "source": [
    "## Explicando a lógica de links ##\n",
    "O próximo passo da simulação é conectar os nós entre si através de enlaces.\n",
    "Cada link possui:\n",
    "- uma origem (src) e um destino (dst)\n",
    "- uma tecnologia de comunicação (LTE, WiFi, fibra, satélite)\n",
    "- uma latência base dependente da camada do nó de origem\n",
    "- uma banda base\n",
    "- uma probabilidade de perda base\n",
    "Esses valores representam condições físicas da infraestrutura antes de qualquer desastre.\n",
    "Depois, durante a fase \"during_disaster\", esses valores serão degradados conforme o cenário.\n",
    "\n",
    "Tecnologias permitidas por tipo de link\n",
    "Origem → Destino\tTecnologias possíveis\n",
    "EDGE → FOG\tLTE, WiFi, LoRaWAN\n",
    "FOG → CLOUD\tfibra, micro-ondas\n",
    "EDGE → CLOUD\tsatélite, LTE\n",
    "\n",
    "Parâmetros base por tecnologia\n",
    "(Valores inspirados em medições reais de redes móveis, WiFi e fibra)\n",
    "Tecnologia Latência base (ms)\tPerda base Banda típica (Mbps)\n",
    "LTE\t30–70\t0.5–5%\t10–50\n",
    "WiFi\t5–20\t0.1–2%\t50–300\n",
    "LoRaWAN\t50–150\t1–5%\t0.02–0.05 (muito baixa)\n",
    "fibra\t1–5\t<0.1%\t1000–10000\n",
    "micro-ondas\t5–15\t0.1–1%\t200–1000\n",
    "satélite\t150–700\t1–10%\t5–50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "893af19e-e1b3-48bc-bf4e-a7a0adb20b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_links_topology(nodes, n_links):\n",
    "    \"\"\"\n",
    "    Gera uma topologia minimalista porém sempre conectada:\n",
    "\n",
    "    - Cada nó EDGE é conectado a exatamente 1 FOG.\n",
    "    - Cada nó FOG é conectado a pelo menos 1 CLOUD (se clouds existirem).\n",
    "    - Se n_links for maior que o mínimo necessário, links extras são\n",
    "      adicionados, sempre respeitando o padrão:\n",
    "          EDGE -> FOG   ou   FOG -> CLOUD.\n",
    "\n",
    "    Se n_links for menor que o mínimo necessário para garantir conectividade,\n",
    "    o gerador ainda criará todos os links mínimos necessários e, na prática,\n",
    "    ignorará o n_links mais restritivo (priorizando a arquitetura coerente).\n",
    "    \"\"\"\n",
    "\n",
    "    edges  = nodes[nodes[\"layer\"] == \"edge\"][\"node_id\"].tolist()\n",
    "    fogs   = nodes[nodes[\"layer\"] == \"fog\"][\"node_id\"].tolist()\n",
    "    clouds = nodes[nodes[\"layer\"] == \"cloud\"][\"node_id\"].tolist()\n",
    "\n",
    "    rows = []\n",
    "    pairs = []\n",
    "\n",
    "    # ---------------\n",
    "    # Casos degenerados\n",
    "    # ---------------\n",
    "    if not fogs:\n",
    "        # Sem FOG não há muito o que fazer em termos de arquitetura E–F–C.\n",
    "        # Poderíamos lançar uma exceção, mas por ora retornamos DataFrame vazio.\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"link_id\",\"src_node_id\",\"dst_node_id\",\"tech\",\n",
    "            \"base_latency_ms\",\"base_bandwidth_mbps\",\"base_loss_rate\"\n",
    "        ])\n",
    "\n",
    "    # ---------------\n",
    "    # 1) Conectar cada EDGE a exatamente 1 FOG\n",
    "    # ---------------\n",
    "    for e in edges:\n",
    "        f = random.choice(fogs)\n",
    "        pairs.append((e, f))\n",
    "\n",
    "    # ---------------\n",
    "    # 2) Conectar cada FOG a pelo menos 1 CLOUD (se existirem clouds)\n",
    "    # ---------------\n",
    "    if clouds:\n",
    "        for f in fogs:\n",
    "            c = random.choice(clouds)\n",
    "            pairs.append((f, c))\n",
    "\n",
    "    # Remover duplicatas mantendo a ordem\n",
    "    seen = set()\n",
    "    unique_pairs = []\n",
    "    for src, dst in pairs:\n",
    "        if (src, dst) not in seen:\n",
    "            seen.add((src, dst))\n",
    "            unique_pairs.append((src, dst))\n",
    "    pairs = unique_pairs\n",
    "\n",
    "    min_required = len(pairs)\n",
    "\n",
    "    # ---------------\n",
    "    # 3) Se o usuário pediu mais links (n_links > min_required),\n",
    "    #    adicionamos links extras válidos (EDGE–FOG, FOG–CLOUD).\n",
    "    # ---------------\n",
    "    # Construir todos os pares válidos possíveis E–F e F–C\n",
    "    all_pairs = []\n",
    "\n",
    "    # EDGE -> FOG\n",
    "    for e in edges:\n",
    "        for f in fogs:\n",
    "            all_pairs.append((e, f))\n",
    "\n",
    "    # FOG -> CLOUD\n",
    "    for f in fogs:\n",
    "        for c in clouds:\n",
    "            all_pairs.append((f, c))\n",
    "\n",
    "    # Filtrar os que ainda não estão em pairs\n",
    "    remaining = [p for p in all_pairs if p not in seen]\n",
    "    random.shuffle(remaining)\n",
    "\n",
    "    # Se n_links for menor que min_required, priorizamos min_required\n",
    "    target_links = max(n_links, min_required)\n",
    "\n",
    "    for p in remaining:\n",
    "        if len(pairs) >= target_links:\n",
    "            break\n",
    "        pairs.append(p)\n",
    "        seen.add(p)\n",
    "\n",
    "    # ---------------\n",
    "    # 4) Geração das métricas de cada link\n",
    "    # ---------------\n",
    "    rows = []\n",
    "    for i, (src, dst) in enumerate(pairs, start=1):\n",
    "\n",
    "        # Acesso EDGE -> FOG\n",
    "        if src.startswith(\"E\") and dst.startswith(\"F\"):\n",
    "            tech = random.choice([\"LTE\", \"WiFi\", \"LoRaWAN\"])\n",
    "            base_latency = np.random.uniform(15, 60)\n",
    "            base_bw      = np.random.uniform(5, 100)\n",
    "            base_loss    = np.random.uniform(0.005, 0.05)\n",
    "\n",
    "        # Backhaul FOG -> CLOUD\n",
    "        elif src.startswith(\"F\") and dst.startswith(\"C\"):\n",
    "            tech = random.choice([\"fibra\", \"microondas\"])\n",
    "            base_latency = np.random.uniform(3, 20)\n",
    "            base_bw      = np.random.uniform(200, 2000)\n",
    "            base_loss    = np.random.uniform(0.0001, 0.01)\n",
    "\n",
    "        else:\n",
    "            # Fallback (não deveria acontecer nesta topologia E–F–C)\n",
    "            tech = \"LTE\"\n",
    "            base_latency = np.random.uniform(20, 80)\n",
    "            base_bw      = np.random.uniform(5, 50)\n",
    "            base_loss    = np.random.uniform(0.01, 0.1)\n",
    "\n",
    "        rows.append([\n",
    "            f\"L{i}\", src, dst, tech,\n",
    "            base_latency, base_bw, base_loss\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"link_id\",\"src_node_id\",\"dst_node_id\",\"tech\",\n",
    "        \"base_latency_ms\",\"base_bandwidth_mbps\",\"base_loss_rate\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ad169-c13e-4bf8-ab08-99b5f17e1db7",
   "metadata": {},
   "source": [
    "## Fases temporais da simulação ##\n",
    "A simulação completa se divide em até três fases:\n",
    "- pre_disaster\n",
    "- during_disaster\n",
    "- post_disaster\n",
    "A função abaixo é usada em vários módulos para decidir se o timestamp t:\n",
    "1) está antes do desastre,\n",
    "2) dentro do intervalo do desastre,\n",
    "ou 3) após o desastre.\n",
    "Essa resposta determina como os modelos probabilísticos serão aplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77e798ea-8024-48f5-9e71-c22f25bca464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_from_time(t, cfg):\n",
    "    if not cfg[\"disaster\"]:\n",
    "        return \"normal\"\n",
    "    if t < cfg[\"disaster_start\"]:\n",
    "        return \"pre_disaster\"\n",
    "    if t < cfg[\"disaster_end\"]:\n",
    "        return \"during_disaster\"\n",
    "    return \"post_disaster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "657e0629-00cc-41e8-8915-a3646928f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_neighbors(links):\n",
    "    neighbors = {}\n",
    "    for _, row in links.iterrows():\n",
    "        u = row[\"src_node_id\"]\n",
    "        v = row[\"dst_node_id\"]\n",
    "        neighbors.setdefault(u, set()).add(v)\n",
    "        neighbors.setdefault(v, set()).add(u)\n",
    "    return neighbors\n",
    "\n",
    "def build_link_index(links):\n",
    "    idx = {}\n",
    "    for _, row in links.iterrows():\n",
    "        u = row[\"src_node_id\"]\n",
    "        v = row[\"dst_node_id\"]\n",
    "        lid = row[\"link_id\"]\n",
    "        idx[(u, v)] = lid\n",
    "        idx[(v, u)] = lid\n",
    "    return idx\n",
    "\n",
    "def compute_flow_path(flow, links, nodes):\n",
    "    src = flow[\"src_node_id\"]\n",
    "    dst = flow[\"dst_node_id\"]\n",
    "\n",
    "    link_index = build_link_index(links)\n",
    "\n",
    "    # EDGE -> FOG\n",
    "    if src.startswith(\"E\") and dst.startswith(\"F\"):\n",
    "        return link_index.get((src, dst), \"\")\n",
    "\n",
    "    # EDGE -> CLOUD (via FOG)\n",
    "    if src.startswith(\"E\") and dst.startswith(\"C\"):\n",
    "        fogs = nodes[nodes[\"layer\"] == \"fog\"][\"node_id\"].tolist()\n",
    "        for f in fogs:\n",
    "            if (src, f) in link_index and (f, dst) in link_index:\n",
    "                return f\"{link_index[(src, f)]}>{link_index[(f, dst)]}\"\n",
    "\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a57d7-af1b-497b-8e3c-430af0e365c9",
   "metadata": {},
   "source": [
    "Nesta parte do notebook, analisaremos a função responsável por gerar a série temporal de métricas de cada enlace da rede (links).\n",
    "Essa série temporal é uma das partes mais importantes do dataset, pois modela:\n",
    "- latência,\n",
    "- jitter,\n",
    "- perda,\n",
    "- throughput,\n",
    "- ocupação de fila,\n",
    "- queda do link (is_up),\n",
    "- causa da degradação.\n",
    "O comportamento dessas métricas muda dependendo do cenário (C1–C5) e dependendo da fase temporal (pre, during, post-disaster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58164917-2c85-4248-9efc-bb4c518d4145",
   "metadata": {},
   "source": [
    "## Objetivo da função generate_link_timeseries() ##\n",
    "Essa função:\n",
    "- recebe a tabela de links (topologia estática),\n",
    "- aplica regras e distribuições estatísticas para cada timestamp,\n",
    "- gera efeitos de desastre, flapping, congestionamento etc.,\n",
    "- devolve uma tabela longa com uma linha por (link × timestamp).\n",
    "A saída possui dezenas, centenas ou milhares de linhas, dependendo dos parâmetros de duração e step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b734a456-25b4-42e7-97f7-d96ef2a9ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_link_timeseries(links, cfg, duration, step):\n",
    "    \"\"\"\n",
    "    Gera séries temporais dos links.\n",
    "\n",
    "    Correções importantes:\n",
    "    - latência e jitter nunca ficam negativos (truncamento físico);\n",
    "    - toda a degradação (desastre, flapping, congestionamento) é aplicada\n",
    "      diretamente nos links, que serão a base para os fluxos.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamps = np.arange(0, duration, step)\n",
    "    rows = []\n",
    "\n",
    "    for _, link in links.iterrows():\n",
    "        for t in timestamps:\n",
    "\n",
    "            phase = phase_from_time(t, cfg)\n",
    "\n",
    "            # Estado base (rede \"normal\")\n",
    "            latency = max(0.1, np.random.normal(link[\"base_latency_ms\"], 3))\n",
    "            jitter  = max(0.1, np.random.normal(2, 1))\n",
    "            loss    = np.random.uniform(0, link[\"base_loss_rate\"])\n",
    "            is_up   = 1\n",
    "            cause   = \"none\"\n",
    "\n",
    "            # -------- Fase de desastre --------\n",
    "            if not cfg[\"stable\"]:\n",
    "                if phase == \"during_disaster\":\n",
    "                    latency *= np.random.uniform(1.5, 3.5)\n",
    "                    jitter  *= np.random.uniform(1.2, 1.8)\n",
    "                    loss    += np.random.uniform(0.05, 0.25)\n",
    "                    cause    = \"infra_danificada\"\n",
    "\n",
    "                    # Flapping (C5)\n",
    "                    if cfg[\"flapping\"]:\n",
    "                        if (t // (step * 3)) % 2 == 0 and random.random() < 0.3:\n",
    "                            is_up = 0\n",
    "                            loss  = 1.0\n",
    "                            latency *= 2\n",
    "                            jitter  *= 2\n",
    "                    else:\n",
    "                        # Queda ocasional de link (C2)\n",
    "                        if random.random() < 0.1:\n",
    "                            is_up = 0\n",
    "                            loss  = 1.0\n",
    "\n",
    "                # -------- Pós-desastre --------\n",
    "                elif phase == \"post_disaster\":\n",
    "                    if cfg[\"adaptive_recovery\"]:  # C3\n",
    "                        latency *= np.random.uniform(0.8, 1.2)\n",
    "                        jitter  *= np.random.uniform(0.8, 1.2)\n",
    "                        loss = max(0.0, loss - np.random.uniform(0.02, 0.08))\n",
    "                        cause = \"recuperacao\"\n",
    "                    else:\n",
    "                        latency *= np.random.uniform(0.9, 1.3)\n",
    "                        jitter  *= np.random.uniform(0.9, 1.4)\n",
    "                        loss = min(1.0, loss + np.random.uniform(0.0, 0.05))\n",
    "\n",
    "            # -------- Congestionamento (C4) --------\n",
    "            if cfg[\"high_traffic\"]:\n",
    "                loss   += np.random.uniform(0.02, 0.12)\n",
    "                jitter *= np.random.uniform(1.1, 1.5)\n",
    "                if phase != \"pre_disaster\":\n",
    "                    cause = \"congestionamento\"\n",
    "\n",
    "            # Normalizações finais\n",
    "            latency = max(0.1, latency)\n",
    "            jitter  = max(0.1, jitter)\n",
    "            loss    = min(max(loss, 0.0), 1.0)\n",
    "\n",
    "            throughput = (\n",
    "                np.random.uniform(0.5, 1.0)\n",
    "                * link[\"base_bandwidth_mbps\"]\n",
    "                * (1 if is_up else 0)\n",
    "            )\n",
    "\n",
    "            queue_occ = (\n",
    "                np.random.uniform(0.3, 1.0)\n",
    "                if cfg[\"high_traffic\"]\n",
    "                else np.random.uniform(0.0, 0.3)\n",
    "            )\n",
    "\n",
    "            rows.append([\n",
    "                t, link[\"link_id\"], phase,\n",
    "                latency, jitter, loss,\n",
    "                throughput, queue_occ,\n",
    "                is_up, cause\n",
    "            ])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"timestamp\",\"link_id\",\"phase\",\n",
    "        \"latency_ms\",\"jitter_ms\",\"loss_rate\",\n",
    "        \"throughput_mbps\",\"queue_occupancy\",\n",
    "        \"is_up\",\"degradation_cause\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e6721-5be7-4009-ba73-c74a14379198",
   "metadata": {},
   "source": [
    "## Introdução aos próximos geradores: ##\n",
    "Nesta etapa, estudaremos três geradores essenciais:\n",
    "1. generate_flows()\n",
    "Responsável por criar os fluxos lógicos entre nós da rede, modelados como:\n",
    "telemetria crítica,\n",
    "controle de atuadores,\n",
    "vídeo de drones,\n",
    "alertas à população,\n",
    "logs em lote,\n",
    "best-effort.\n",
    "Esses fluxos possuem requisitos diferentes: latência, confiabilidade, prioridade e padrões de tráfego.\n",
    "2. generate_events()\n",
    "Simula eventos externos como:\n",
    "terremoto,\n",
    "aftershock,\n",
    "quedas de links,\n",
    "flapping.\n",
    "Eventos são fundamentais para cenários C2–C5.\n",
    "3. generate_control_actions()\n",
    "Simula ações tomadas pelo sistema (fog ou cloud) para restaurar SLA, como:\n",
    "reroteamento de fluxo,\n",
    "alteração de prioridade,\n",
    "redução de carga (throttle),\n",
    "migração de serviço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2453fa1c-a540-4105-ac88-fcb18e19de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flows(nodes, links, cfg, duration):\n",
    "    app_profiles = {\n",
    "        \"telemetria_critica\": (\"telemetria_critica\", 1, 150, 0.99),\n",
    "        \"video_drone\":        (\"video_drone\",        2, 300, 0.95),\n",
    "        \"best_effort\":        (\"best_effort\",        3, 800, 0.90),\n",
    "    }\n",
    "\n",
    "    fogs = nodes[nodes[\"layer\"]==\"fog\"][\"node_id\"].tolist()\n",
    "    clouds = nodes[nodes[\"layer\"]==\"cloud\"][\"node_id\"].tolist()\n",
    "    neighbors = build_neighbors(links)\n",
    "\n",
    "    rows = []\n",
    "    fid = 1\n",
    "\n",
    "    for _, row in nodes[nodes[\"layer\"]==\"edge\"].iterrows():\n",
    "        src = row[\"node_id\"]\n",
    "        dtype = row[\"device_type\"]\n",
    "\n",
    "        fog_neighbors = [n for n in neighbors.get(src,[]) if n in fogs]\n",
    "\n",
    "        cloud_candidates = []\n",
    "        for f in fog_neighbors:\n",
    "            for c in neighbors.get(f,[]):\n",
    "                if c in clouds:\n",
    "                    cloud_candidates.append((f,c))\n",
    "\n",
    "        if dtype == \"sensor\" and fog_neighbors:\n",
    "            app,prio,lat,rel = app_profiles[\"telemetria_critica\"]\n",
    "            dst = fog_neighbors[0]\n",
    "            rows.append([f\"FL{fid}\",src,dst,app,prio,lat,rel,\"constante\",0,duration])\n",
    "            fid+=1\n",
    "\n",
    "        elif dtype == \"UAV\" and (fog_neighbors or cloud_candidates):\n",
    "            app,prio,lat,rel = app_profiles[\"video_drone\"]\n",
    "            if fog_neighbors and (not cloud_candidates or random.random()<0.5):\n",
    "                dst = fog_neighbors[0]\n",
    "            else:\n",
    "                _,dst = cloud_candidates[0]\n",
    "            rows.append([f\"FL{fid}\",src,dst,app,prio,lat,rel,\"bursty\",0,duration])\n",
    "            fid+=1\n",
    "\n",
    "        elif dtype == \"wearable\" and cloud_candidates:\n",
    "            app,prio,lat,rel = app_profiles[\"best_effort\"]\n",
    "            _,dst = cloud_candidates[0]\n",
    "            rows.append([f\"FL{fid}\",src,dst,app,prio,lat,rel,\"on_off\",0,duration])\n",
    "            fid+=1\n",
    "\n",
    "    flows = pd.DataFrame(rows, columns=[\n",
    "        \"flow_id\",\"src_node_id\",\"dst_node_id\",\"app_type\",\"priority\",\n",
    "        \"required_latency_ms\",\"required_reliability\",\n",
    "        \"traffic_pattern\",\"start_time\",\"end_time\"\n",
    "    ])\n",
    "\n",
    "    if cfg[\"high_traffic\"]:\n",
    "        flows = pd.concat([flows, flows.sample(frac=1, replace=True)], ignore_index=True)\n",
    "\n",
    "    return flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643ee4d-03bb-4003-acf3-cb05df7907f4",
   "metadata": {},
   "source": [
    "## Explicando o gerador de eventos ##\n",
    "Eventos representam ocorrências externas que afetam a rede.\n",
    "No ITERATION-D, consideramos:\n",
    "- terremoto (impacto global),\n",
    "- aftershock (impacto global de menor intensidade),\n",
    "- quedas de links individuais,\n",
    "- flapping em cenários específicos.\n",
    "Esses eventos servem como gatilhos para degradação e ações de controle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eea5074-29d5-4110-9744-b0a3ffe133fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_events(links, cfg, duration):\n",
    "    rows = []\n",
    "    eid = 1\n",
    "\n",
    "    if cfg[\"disaster\"]:\n",
    "        rows.append([f\"E{eid}\", cfg[\"disaster_start\"] - 10,\n",
    "                     \"earthquake_shock\", \"region\", \"global\", 5])\n",
    "        eid += 1\n",
    "\n",
    "        rows.append([f\"E{eid}\", cfg[\"disaster_start\"] + 300,\n",
    "                     \"aftershock\", \"region\", \"global\", 3])\n",
    "        eid += 1\n",
    "\n",
    "        affected = random.sample(links[\"link_id\"].tolist(),\n",
    "                                 max(1, len(links)//5))\n",
    "\n",
    "        for lid in affected:\n",
    "            t = random.randint(cfg[\"disaster_start\"], cfg[\"disaster_end\"])\n",
    "            rows.append([f\"E{eid}\", t, \"link_down\", \"link\", lid, 4])\n",
    "            eid += 1\n",
    "\n",
    "        if cfg[\"flapping\"]:\n",
    "            for lid in affected[: len(affected)//2 ]:\n",
    "                t = random.randint(cfg[\"disaster_start\"], cfg[\"disaster_end\"])\n",
    "                rows.append([f\"E{eid}\", t, \"link_flapping\", \"link\", lid, 2])\n",
    "                eid += 1\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"event_id\",\"timestamp\",\"type\",\"target_type\",\"target_id\",\"severity\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8aef1-4b63-4db9-911a-01bd77272e99",
   "metadata": {},
   "source": [
    "## O gerador de ações de controle ##\n",
    "A função generate_control_actions() simula o que o sistema tenta fazer para restaurar SLA após o desastre:\n",
    "- reroute_flow → mudar caminho\n",
    "- promote_flow_priority → aumentar prioridade\n",
    "- throttle_video → reduzir tráfego de vídeo\n",
    "- migrate_service → mover serviço para outro nó\n",
    "  \n",
    "Essas ações representam algumas estratégias adaptativas que podem ser implementadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71692e2-f7c7-4000-a3c0-2d4a970d2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_control_actions(flows, cfg):\n",
    "    rows = []\n",
    "    aid = 1\n",
    "\n",
    "    if not cfg[\"disaster\"]:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"action_id\",\"timestamp\",\"controller_layer\",\n",
    "            \"action_type\",\"target_id\",\"expected_effect\",\n",
    "            \"observed_effect_sla\"\n",
    "        ])\n",
    "\n",
    "    sample_flows = flows[\"flow_id\"].tolist()[:10]\n",
    "\n",
    "    for f in sample_flows:\n",
    "        rows.append([\n",
    "            f\"A{aid}\",\n",
    "            cfg[\"disaster_start\"] + cfg[\"disaster_duration\"]//2,\n",
    "            random.choice([\"fog\", \"cloud\"]),\n",
    "            random.choice([\"reroute_flow\",\"promote_flow_priority\",\n",
    "                           \"throttle_video\",\"migrate_service\"]),\n",
    "            f,\n",
    "            \"restore_SLA\",\n",
    "            random.choice([\"improved\", \"neutral\"])\n",
    "        ])\n",
    "        aid += 1\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"action_id\",\"timestamp\",\"controller_layer\",\"action_type\",\n",
    "        \"target_id\",\"expected_effect\",\"observed_effect_sla\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a505a4-7df3-4c76-bd55-7b341cabb9a8",
   "metadata": {},
   "source": [
    "Nesta parte, estudaremos a função mais “alto nível” do simulador: generate_flow_timeseries()\n",
    "Ela produz, para cada fluxo da rede:\n",
    "- latência fim-a-fim,\n",
    "- jitter,\n",
    "- bytes entregues,\n",
    "- pacotes descartados,\n",
    "- cumprimento de SLA,\n",
    "- caminho lógico (path_links),\n",
    "- fase temporal da simulação.\n",
    "  \n",
    "Ao contrário da série temporal de links, que trabalha por enlace, esta função trabalha por fluxo, integrando:\n",
    "\n",
    "- comportamento da aplicação,\n",
    "- requisitos de SLA,\n",
    "- impacto do desastre,\n",
    "- degradação acumulada,\n",
    "- possíveis efeitos de congestionamento ou flapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42405d3b-6ddb-4ac2-9f33-e09cfb83ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flow_timeseries(flows, links, nodes, link_timeseries, cfg, duration, step):\n",
    "    \"\"\"\n",
    "    Gera séries temporais dos fluxos *a partir* dos links no caminho.\n",
    "\n",
    "    - path_links vem de compute_flow_path (ex.: \"L1>L3\");\n",
    "    - em cada timestamp t, buscamos as métricas dos links em link_timeseries;\n",
    "    - latência fim a fim  = soma das latências dos links;\n",
    "    - jitter fim a fim    = soma dos jitters dos links;\n",
    "    - perda fim a fim     = 1 - ∏ (1 - loss_i);\n",
    "    - throughput_mbps     = min(throughput_mbps dos links)  (gargalo);\n",
    "    - delivered/dropped   = derivados de throughput + perda.\n",
    "\n",
    "    A saída inclui tanto throughput em Mbps quanto bytes por intervalo.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamps = np.arange(0, duration, step)\n",
    "\n",
    "    # Precomputar caminhos de cada fluxo\n",
    "    flow_paths_str = {}\n",
    "    flow_paths_list = {}\n",
    "\n",
    "    for _, flow in flows.iterrows():\n",
    "        path_str = compute_flow_path(flow, links, nodes)  # ex: \"L1>L3\"\n",
    "        flow_paths_str[flow[\"flow_id\"]] = path_str\n",
    "        if path_str:\n",
    "            flow_paths_list[flow[\"flow_id\"]] = path_str.split(\">\")\n",
    "        else:\n",
    "            flow_paths_list[flow[\"flow_id\"]] = []\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, flow in flows.iterrows():\n",
    "        flow_id = flow[\"flow_id\"]\n",
    "        path_links = flow_paths_list[flow_id]\n",
    "\n",
    "        # Se o fluxo não tem caminho válido, pulamos\n",
    "        if not path_links:\n",
    "            continue\n",
    "\n",
    "        required_lat = flow[\"required_latency_ms\"]\n",
    "        required_rel = flow[\"required_reliability\"]\n",
    "\n",
    "        for t in timestamps:\n",
    "\n",
    "            if not (flow[\"start_time\"] <= t <= flow[\"end_time\"]):\n",
    "                continue\n",
    "\n",
    "            phase = phase_from_time(t, cfg)\n",
    "\n",
    "            lat_list  = []\n",
    "            jit_list  = []\n",
    "            loss_list = []\n",
    "            thr_list  = []\n",
    "\n",
    "            # Coleta métricas de cada link do caminho\n",
    "            for lid in path_links:\n",
    "                link_row = link_timeseries[\n",
    "                    (link_timeseries[\"link_id\"] == lid) &\n",
    "                    (link_timeseries[\"timestamp\"] == t)\n",
    "                ]\n",
    "\n",
    "                if link_row.empty:\n",
    "                    continue\n",
    "\n",
    "                r = link_row.iloc[0]\n",
    "                lat_list.append(r[\"latency_ms\"])\n",
    "                jit_list.append(r[\"jitter_ms\"])\n",
    "                loss_list.append(r[\"loss_rate\"])\n",
    "                thr_list.append(r[\"throughput_mbps\"])\n",
    "\n",
    "            # Se não achamos nenhum dado de link para esse t, ignoramos\n",
    "            if not lat_list:\n",
    "                continue\n",
    "\n",
    "            # -------------------------\n",
    "            # Composição fim a fim\n",
    "            # -------------------------\n",
    "\n",
    "            # Latência/jitter: soma\n",
    "            lat_e2e = sum(lat_list)\n",
    "            jit_e2e = sum(jit_list)\n",
    "\n",
    "            # Perda composta: 1 - ∏(1 - loss_i)\n",
    "            success_prob = 1.0\n",
    "            for li in loss_list:\n",
    "                success_prob *= (1.0 - li)\n",
    "            loss_e2e = 1.0 - success_prob\n",
    "            loss_e2e = min(max(loss_e2e, 0.0), 1.0)\n",
    "\n",
    "            # Throughput fim a fim (gargalo do caminho)\n",
    "            if thr_list:\n",
    "                thr_e2e_mbps = max(0.0, min(thr_list))\n",
    "            else:\n",
    "                thr_e2e_mbps = 0.0\n",
    "\n",
    "            # Bytes entregues/descartados no intervalo\n",
    "            # throughput (Mbps) -> bytes/s -> bytes/intervalo\n",
    "            # 1 Mbps = 1e6 bits/s = 1e6/8 bytes/s\n",
    "\n",
    "            bytes_per_sec = (thr_e2e_mbps * 1e6) / 8.0\n",
    "            interval_seconds = step\n",
    "\n",
    "            total_bytes_interval = bytes_per_sec * interval_seconds\n",
    "\n",
    "            delivered_bytes = total_bytes_interval * (1.0 - loss_e2e)\n",
    "            dropped_bytes   = total_bytes_interval * loss_e2e\n",
    "\n",
    "            # Para manter valores \"arrumados\"\n",
    "            delivered_bytes = max(0.0, delivered_bytes)\n",
    "            dropped_bytes   = max(0.0, dropped_bytes)\n",
    "\n",
    "            # -------------------------\n",
    "            # SLA\n",
    "            # -------------------------\n",
    "            sla_met = int(\n",
    "                (lat_e2e <= required_lat) and\n",
    "                ((1.0 - loss_e2e) >= required_rel)\n",
    "            )\n",
    "\n",
    "            rows.append([\n",
    "                t,\n",
    "                flow_id,\n",
    "                flow_paths_str[flow_id],    # \"L1>L3\"\n",
    "                lat_e2e,\n",
    "                jit_e2e,\n",
    "                thr_e2e_mbps,\n",
    "                delivered_bytes,\n",
    "                dropped_bytes,\n",
    "                sla_met,\n",
    "                phase\n",
    "            ])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"timestamp\",\n",
    "        \"flow_id\",\n",
    "        \"path_links\",\n",
    "        \"end_to_end_latency_ms\",\n",
    "        \"end_to_end_jitter_ms\",\n",
    "        \"throughput_mbps\",            # throughput E2E no fluxo\n",
    "        \"delivered_bytes_interval\",   # bytes efetivamente entregues no intervalo\n",
    "        \"dropped_bytes_interval\",     # bytes \"perdidos\" no intervalo\n",
    "        \"sla_met\",\n",
    "        \"phase\"\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdafd82-6e6c-4316-8050-72dc98d6ebac",
   "metadata": {},
   "source": [
    "## O papel da run_generator ##\n",
    "Até agora, vimos vários blocos da ferramenta:\n",
    "- get_scenario_config → define o comportamento global do cenário (C1–C5).\n",
    "- generate_nodes → cria a topologia de nós (edge, fog, cloud).\n",
    "- generate_links (ou generate_links_realistic) → cria os enlaces com latência, perda e banda base.\n",
    "- generate_link_timeseries → gera as métricas de rede por link ao longo do tempo.\n",
    "- generate_flows → cria os fluxos lógicos (aplicações).\n",
    "- generate_events → introduz terremotos, aftershocks, quedas de link, flapping.\n",
    "- generate_control_actions → simula as ações de controle (fog/cloud).\n",
    "- generate_flow_timeseries → gera métricas fim-a-fim por fluxo.\n",
    "A função run_generator é o “orquestrador principal”:\n",
    "Ela chama todas as outras funções na ordem correta.\n",
    "Garante reprodutibilidade (via seed).\n",
    "Retorna tudo pronto, em um único dicionário de DataFrames — ideal para Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9dc4c7b7-b358-4121-927c-55bb8ef6e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generator(\n",
    "    scenario,\n",
    "    n_edge,\n",
    "    n_fog,\n",
    "    n_cloud,\n",
    "    n_links,\n",
    "    duration,\n",
    "    step,\n",
    "    seed\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera todos os DataFrames do cenário ITERATION-D e retorna em um dicionário.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    cfg = get_scenario_config(scenario, duration)\n",
    "\n",
    "    # Topologia\n",
    "    nodes = generate_nodes(n_edge, n_fog, n_cloud)\n",
    "    links = generate_links_topology(nodes, n_links)\n",
    "\n",
    "    # Fluxos (definidos com base na topologia)\n",
    "    flows = generate_flows(nodes, links, cfg, duration)\n",
    "\n",
    "    # Séries temporais de links\n",
    "    link_ts = generate_link_timeseries(links, cfg, duration, step)\n",
    "\n",
    "    # Eventos e ações\n",
    "    events  = generate_events(links, cfg, duration)\n",
    "    actions = generate_control_actions(flows, cfg)\n",
    "\n",
    "    # Séries temporais de fluxos — agora baseadas em link_ts\n",
    "    flow_ts = generate_flow_timeseries(flows, links, nodes, link_ts, cfg, duration, step)\n",
    "\n",
    "    return {\n",
    "        \"nodes\": nodes,\n",
    "        \"links\": links,\n",
    "        \"flows\": flows,\n",
    "        \"link_timeseries\": link_ts,\n",
    "        \"flow_timeseries\": flow_ts,\n",
    "        \"events\": events,\n",
    "        \"control_actions\": actions,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f4466-a2f5-43e0-ae2f-d7bd1c8f6dea",
   "metadata": {},
   "source": [
    "Agora que todas as funções internas já foram definidas, vamos finalmente executar o gerador completo usando run_generator().\n",
    "\n",
    "No bloco abaixo:\n",
    "- Executamos a simulação para um cenário específico (C1–C5).\n",
    "- Recebemos os sete DataFrames gerados:\n",
    "    - nodes\n",
    "    - links\n",
    "    - link_timeseries\n",
    "    - flows\n",
    "    - flow_timeseries\n",
    "    - events\n",
    "    - control_actions\n",
    "      \n",
    "Salvamos todos esses DataFrames em arquivos .csv com nomes padronizados, facilitando processamento e análise posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad84fdb0-91cd-4114-9d78-4a618f4daf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o gerador sintético de dados para o cenário escolhido\n",
    "\n",
    "scenario = \"C2\"\n",
    "\n",
    "data = run_generator(\n",
    "    scenario=scenario,   # Cenário da simulação (C1=estável, C2=desastre, C3=recuperação,\n",
    "                     # C4=saturação urbana, C5=flapping)\n",
    "    \n",
    "    n_edge=20,       # Número de dispositivos na camada EDGE\n",
    "    n_fog=5,         # Número de nós intermediários na camada FOG\n",
    "    n_cloud=1,       # Número de datacenters na camada CLOUD\n",
    "\n",
    "    n_links=25,      # Número total de enlaces a serem gerados\n",
    "                \n",
    "\n",
    "    duration=3600,   # Duração total da simulação (em segundos)\n",
    "    step=5,         # Intervalo entre pontos da série temporal (segundos)\n",
    "\n",
    "    seed=123         # Seed para garantir reprodutibilidade\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac5f18a8-876a-4495-9515-076ce4510f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo: iterationD_nodes_C2.csv\n",
      "Arquivo salvo: iterationD_links_C2.csv\n",
      "Arquivo salvo: iterationD_flows_C2.csv\n",
      "Arquivo salvo: iterationD_link_timeseries_C2.csv\n",
      "Arquivo salvo: iterationD_flow_timeseries_C2.csv\n",
      "Arquivo salvo: iterationD_events_C2.csv\n",
      "Arquivo salvo: iterationD_control_actions_C2.csv\n"
     ]
    }
   ],
   "source": [
    "# Salvamento automático de todos os arquivos gerados\n",
    "# Cada chave no dicionário 'data' vira um arquivo CSV.\n",
    "\n",
    "for name, df in data.items():\n",
    "\n",
    "    # O nome do arquivo segue o padrão:\n",
    "    #   iterationD_<nome do dataframe>_<cenário>.csv\n",
    "    filename = f\"iterationD_{name}_{scenario}.csv\"\n",
    "\n",
    "    # Exporta o DataFrame para CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    # Log informativo\n",
    "    print(\"Arquivo salvo:\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53f8bd-16ba-461b-bddd-7338fbfb6df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
